# CS 361 Review Notes

Here is a simple review notes which cover the concepts and skills of CS 361.



---

# Linearity Property for Common Parameter


### Mean
- **Linearity**: $\bar{x}_{new} = \bar{x} + c$, where $\bar{x}$ is the mean of $\{x_i\}$ and $\bar{x}_{new}$ is the mean of $\{x_i + c\}$.
- **Scaling**: $\bar{x}_{new} = c\bar{x}$, where $\bar{x}$ is the mean of $\{x_i\}$ and $\bar{x}_{new}$ is the mean of $\{cx_i\}$.

### Median
- **Scaling**: $\text{Median}_{new} = c \cdot \text{Median}$, where the Median is of $\{x_i\}$ and $\text{Median}_{new}$ is the median of $\{cx_i\}$.

### Mode
- No simple linearity or scaling properties; mode is based on frequency and not directly affected by transformations.

### Standard Deviation ($\sigma$)
- **Scaling**: $\sigma_{new} = |c|\sigma$, where $\sigma$ is the standard deviation of $\{x_i\}$ and $\sigma_{new}$ is the standard deviation of $\{cx_i\}$.

### Variance ($\sigma^2$)
- **Scaling**: $\sigma^2_{new} = c^2\sigma^2$, where $\sigma^2$ is the variance of $\{x_i\}$ and $\sigma^2_{new}$ is the variance of $\{cx_i\}$.

### Expected Value ($E[X]$)
- **Linearity**: $E[X + c] = E[X] + c$.
- **Scaling**: $E[cX] = cE[X]$.

### Covariance ($\text{Cov}(X, Y)$)
- **Scaling**: $\text{Cov}(aX + b, cY + d) = ac \cdot \text{Cov}(X, Y)$​.



---

# Chapter 1 & 2 - Location Parameters & Scale Parameters

## Two types of parameters

- **Location Parameter:** A location parameter shifts the distribution along the horizontal axis without changing its shape or scale.
  - **Mean, Median, Mode**
- **Scale Parameter:** A scale parameter modifies the spread or scale of the distribution without affecting its central location.
  - **Variance, standard deviation, IQR.**

## Mean

- **Definition:** The average of all data points.
- **Formula:** $$ \mu \text{ (population mean) or } \bar{x} \text{ (sample mean)} = \frac{\sum x_i}{n} $$
- **Properties:**
  - Sensitive to outliers.
  - Used for interval and ratio level of measurement.

## Median
- **Definition:** The middle value when data points are ordered.
- **Properties:**
  - Not affected by outliers.
  - Can be used for ordinal, interval, and ratio levels of measurement.

## Standard Deviation
- **Definition:** A measure of the amount of variation or dispersion of a set of values.
- **Formula:** $$ \sigma \text{ (population standard deviation) or } s \text{ (sample standard deviation)} = \sqrt{\frac{\sum (x_i - \text{mean})^2}{n - 1}} $$
- **Properties:**
  - Square root of variance.
  - Has the same units as the data points.

## Variance
- **Definition:** The average of the squared differences from the Mean.
- **Formula:** $$ \sigma^2 \text{ (population variance) or } s^2 \text{ (sample variance)} = \frac{\sum (x_i - \text{mean})^2}{n - 1} $$
- **Properties:**
  - Measures variability from the mean.
  - Affected by outliers.

## Interquartile Range (IQR)
- **Definition:** A measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles.
- **Formula:** $$ \text{IQR} = Q_3 - Q_1 $$
- **Properties:**
  - Represents the middle 50% of data.
  - Not affected by outliers.

## Standard Coordinates (z-scores)
- **Definition:** Normalized values showing how many standard deviations an element is from the mean.
- **Formula:** $$ z = \frac{x_i - \mu}{\sigma} $$
- **Properties:**
  - Used in standardizing data.
  - Makes data comparable.
  - $\text{prcentage out of k times std}=\frac{1}{k^2}$

## Correlation Coefficient
- **Definition:** A statistical measure that expresses the extent to which two variables are linearly related (direction and strength).
- **Formula:** $$ r = \frac{\sum [(x_i - \bar{x})(y_i - \bar{y})]}{\sqrt{\sum (x_i - \bar{x})^2 \times \sum (y_i - \bar{y})^2}} $$
- **Properties:**
  - Values range between -1 and 1.
  - 0 indicates no linear correlation.
  - +1 indicates a perfect positive linear correlation.
  - -1 indicates a perfect negative linear correlation.

## Outliers
- **Definition:** An outlier is a data point that differs significantly from other observations.
- **Properties:**
  - Can significantly skew your data.
  - Important to identify and handle them appropriately.

## Correlation's Definition in Standard Coordinates
- **Definition:** Correlation in standard coordinates involves normalizing the values of both variables so that each variable has a mean of 0 and standard deviation of 1 before calculating the correlation coefficient.

## Linear Prediction
- **Definition:** The correlation coefficient can be used to predict the value of one variable based on the value of another, given that there is a linear relationship between the two.
- **Linear Predictor Formula (Regression Equation):**
  - Simple linear regression: $\hat{y}^p =\hat{x}\times r$  where r is the correlation coefficient.
- **Properties:**
  - The strength of the correlation coefficient indicates the reliability of the prediction.
  - The sign indicates the direction of the relationship (positive or negative).



---

# Chapter 3 - Probability

## Permutation and Combination

- Permutation: $P(n, k) = \frac{n!}{(n-k)!}$​
- Combination: $\binom{n}{k} = \frac{n!}{k!(n-k)!}$

## Outcome
- **Definition:** A possible result of a random experiment.

## Sample Space (S)
- **Definition:** The set of all possible outcomes of a random experiment.
- **Notation:** $ S \text{ or } \Omega $

## Event (E)
- **Definition:** A subset of the sample space. Any collection of outcomes.
- **Notation:** $ E \subset S $

## Probability Axioms
1. **Non-negativity:** $ P(E) \geq 0 $ for any event $ E $.
2. **Additivity:** If $ E_1, E_2, \ldots $ are disjoint events, then $$ P\left(\bigcup_{i=1}^{\infty} E_i\right) = \sum_{i=1}^{\infty} P(E_i) $$.
3. **Normalization:** $ P(S) = 1 $, where $S$ is the sample space.

## Probability as Sets, Booleans
- **Intersection (AND):** $ P(A \cap B) $ is the probability of both events A and B occurring.
- **Union (OR):** $ P(A \cup B) $ is the probability of either event A or B occurring.
- **Complement (NOT):** $ P(A^C) $ is the probability of event A not occurring.

## Disjoint Events
- **Definition:** Events that cannot occur simultaneously.
- **Notation:** $ A \cap B = \emptyset $
- **Property:** For disjoint events $ A $ and $ B $, $ P(A \cup B) = P(A) + P(B) $.

## Computing Probability Using Complement
- **Formula:** $$ P(A^C) = 1 - P(A) $$
- **Usage:** Useful when it's easier to calculate the probability of the complement event.

## Computing Probability Using Union
- **Formula for Two Events:** $$ P(A \cup B) = P(A) + P(B) - P(A \cap B) $$

## Conditional Probability
- **Definition:** The probability of event A given that event B has occurred.
- **Formula:** $ P(A|B) = \frac{P(A \cap B)}{P(B)} $, provided that $ P(B) > 0 $.

## Bayes Rule
- **Formula:** $$ P(A|B) = \frac{P(B|A)P(A)}{P(B)} $$
- **Application:** Allows us to update the probability estimate for A in light of the evidence B.

## Law of Total Probability
- Considering a partition of S into $ B_1, B_2, \ldots, B_n $ and any event A:
- **Formula:** $$ P(A) = \sum_{i=1}^{n} P(A|B_i)P(B_i) $$

## Independence
- **Definition:** Two events A and B are independent if the occurrence of A does not affect the probability of B, and vice versa.
- **Condition:** $$ P(A \cap B) = P(A)P(B) $$
- **Implication:** $$ P(A|B) = P(A) $$ and $$ P(B|A) = P(B) $$



---

# Chapter 4 - Discrete Random Variables

## Discrete Random Variable (X)
- **Definition:** A variable that can take on a countable number of distinct outcomes.
- **Notation:** $ X $

## Probability Distribution of a Random Variable
- **Definition:** A function that gives the probability that a discrete random variable is exactly equal to some value.
- **Notation:** $$ P(X = x) $$

## Cumulative Distribution Function (CDF)
- **Definition:** A function that gives the probability that a random variable is less than or equal to a certain value.
- **Notation/Formulation:** $$ F_X(x) = P(X \leq x) $$

## Joint Distribution of Two Discrete Random Variables (X, Y)
- **Definition:** A function giving the probability that each of two random variables falls within a specific range or set of discrete values.
- **Notation/Formulation:** $ P(X = x \text{ and } Y = y) $ or $ f_{X,Y}(x, y) $

## Marginal Probability of a Random Variable
- **Definition:** The probability distribution of a subset of a collection of random variables.
- **Notation/Formulation:** For variable $ X $, $$ P(X = x) = \sum_y P(X = x \text{ and } Y = y) $$

## Independent Random Variables
- **Definition:** Two random variables $ X $ and $ Y $ are independent if the probability distribution of one variable is not affected by the known outcome of the other variable.
- **Condition:** $$ P(X = x \text{ and } Y = y) = P(X = x)P(Y = y) $$

## Expected Value (EV or E[X])
- **Definition:** A measure of the center of the distribution of the variable. Also known as the mean.
- **Notation/Formulation:** For a discrete random variable $ X $, $$ E[X] = \sum_x x P(X = x) $$

## Variance (Var(X))
- **Definition:** A measure of the spread of the distribution of the variable.
- **Notation/Formulation:** $$ \sigma^2_X = Var(X) = E[(X - E[X])^2] = \sum_x (x - E[X])^2 P(X = x) = E[X^2]-E[X]^2$$

## Covariance (Cov(X, Y))
- **Definition:** A measure of the extent to which two random variables change together.
- **Notation/Formulation:** $$ Cov(X, Y) = E[(X - E[X])(Y - E[Y])] $$

## Markov's Inequality
- **Statement:** For any non-negative random variable $ X $ and $ a > 0 $, $$ P(X \geq a) \leq \frac{E[X]}{a} $$

## Chebyshev's Inequality
- **Statement:** For any random variable $ X $ with finite expected value $ \mu $ and finite non-zero variance $ \sigma^2$, for any $ k > 0 $: $$ P(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2} $$

## Weak Law of Large Numbers
- **Statement:** The sample average of a sequence of iid random variables converges in probability towards the expected value as the sample size goes to infinity.

## Independent Identically Distributed Samples (iid Samples)
- **Definition:** A sequence of random variables where each variable has the same probability distribution as the others and all are mutually independent.

## Property of Expectations
- **Linearity:** $$ E[aX + b] = aE[X] + b $$

## Property of Variance
- **Scaling:** $$ Var(aX) = a^2Var(X) $$
- **For independent variables:** $$ Var(X + Y) = Var(X) + Var(Y) $$

## Property of Covariance
- **Symmetry:** $$ Cov(X, Y) = Cov(Y, X) $$
- **For independent variables:** $$ Cov(X, Y) = 0 $$



---

# Chapter 5 - Specific Formulas for Discrete Distributions

## Uniform Distribution
- **Probability Mass Function (PMF):** $$P(X = x) = 
  \begin{cases} 
    \frac{1}{n} & \text{for } x = a, a+1, \ldots, b-1, b \\
    0 & \text{otherwise} 
  \end{cases}$$
- where \(a\) and \(b\) are the bounds of the distribution and $ n = b - a + 1$.
- **Use Scenario**: When all outcomes are equally likely within the defined range. There are two types of uniform distributions: discrete, where the outcomes are countable, and continuous, where outcomes are uncountable and take on any value in an interval.
- **Example**: Rolling a fair six-sided die. Each face (1 to 6) has an equal chance of landing up. The discrete uniform distribution would apply. For a continuous example, consider a random number generator that produces equally likely numbers between 0 and 1. Here, a continuous uniform distribution is used.
- **Expected Value (E[X]):** $ \frac{a + b}{2} $
- **Variance (Var[X]):** $\frac{(b - a + 1)^2 - 1}{12} $

## Bernoulli Distribution
- **PMF:** $$ P(X = x) = 
  \begin{cases} 
    p & \text{for } x = 1 \\
    1 - p & \text{for } x = 0 
  \end{cases} $$​
- **Use Scenario**: In the simplest case of the Binomial distribution, when there's only one trial (such as a single coin flip or a one-shot event with a success or failure outcome).
- **Example**: An inspection where each item is either defective (failure) or non-defective (success). The outcome of inspecting one item follows a Bernoulli distribution.
- **Expected Value (E[X]):** $ p $
- **Variance (Var[X]):** $ p(1 - p) $

## Geometric Distribution
- **PMF:** $$ P(X = x) = p(1 - p)^{x - 1} $$
- for \(x = 1, 2, 3, ...\).
- **Use Scenario**: You're interested in the number of trials needed to achieve the first success in a series of independent Bernoulli trials (e.g., flipping a coin until you get heads).
- **Example**: Keep drawing cards from a shuffled deck until you draw an ace. The geometric distribution could model the number of draws you need to get your first ace.
- **Expected Value (E[X]):** $ \frac{1}{p} $
- **Variance (Var[X]):** $ \frac{1 - p}{p^2} $

## Binomial Distribution
- **PMF:** $$ P(X = x) = \binom{n}{x} p^x (1 - p)^{n - x} $$
- for \(x = 0, 1, 2, ..., n\).
- **Use Scenario**: When you have a fixed number of independent Bernoulli trials (each with two possible outcomes and a constant probability of success), the binomial distribution gives the probability of a specific number of successes.
- **Example**: If you flip a fair coin 10 times, the Binomial distribution can be used to find the probability of flipping exactly 6 heads.
- **Expected Value (E[X]):** $ np $
- **Variance (Var[X]):** $ np(1 - p) $

## Poisson Distribution
- **PMF:** $$ P(X = x) = \frac{e^{-\lambda} \lambda^x}{x!} $$
- for \(x = 0, 1, 2, ...\).
- **Use Scenario**: When you're looking at the number of times an event happens over a fixed interval of time or space, given that these events occur with a known average rate and independently of the time since the last event.
- **Example**: Counting the number of calls received by a call center in an hour or the number of emails you receive in a day might follow a Poisson distribution.
- **Expected Value (E[X]):** $ \lambda $
- **Variance (Var[X]):** $ \lambda $